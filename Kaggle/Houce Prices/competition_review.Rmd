---
title: "Review on 'House Prices' Kaggle Competition"
author: "Adilet Absatov"
date: 'June 28, 2018'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Competition description

Competition House Prices предлагает поработать с датасетом, состоящим из 1460 объектов и 80 независимых переменных. В числе независымых переменных различные данные о домах: от материала крыши до качества каминов. Согласно этим данным нужно построить оптимальную модель и спрогнозировать цену дома из тестового датасета.

## Data engineering

В исходном тренировочном датасете более 6500 пропущенных значений, поэтому первым делом нужно было избавиться от них.

Для этого были использованы два разных подхода:

* Тщательный анализ каждого столбца, удаление столбцов со множеством повторяющихся значений, добавление новые уровни фактора, осмысленная замена пропущенных значений.
* Удаление столбцов со множеством пропущенных значений, перевод факторных переменных в количественные, замена оставшихся пропущенных значений на медианы рядов или экстраполяция с missForest.

После 3-часовой работы в 180 строк с первым подходом я ожидал превосходного результата, однако, к моему удивлению, второй подход, требовавший 5 минут на реализацию, оказался более эффективным.

Так же были удалены выбросы (которые составляли 20% всех данных) и колонка Id, не имеющая значения для прогнозирования.

![](boxplot.jpg)

## Process

### Linear Model

Реализация:

![](code_lm.jpg)

Хоть и эффективность модели была неплохая, распределение остатков было просто ужасным.
![](sum_lm.jpg)
![](hist_lm.jpg)

Какие либо преобразования для улучшения нормальности распределения делать я не стал, потому что значительная часть переменных была факторами и изменения не сильно улучшили бы ситуацию.

В итоге прогноз по линейной модели дал показатель RMLSE = 0.17503 и я оказался немного не принят в топ 3000.
![](lm_position.jpg)


### RandomForest

Реализация:

![](code_rf.jpg)

С помощью подбора и кроссвалидации я нашёл оптимальное количество деревьев - 350.

В результате я поднялся на 741 строчку с RMLSE = 0.14749.

![](rf_position.jpg)

### xgboost

В отличие от двух предыдущих способов, в этом случае экстраполяция с missForest была более эффективной. Единственным её минусом было долгое исполнение. Так же были сохранены выбросы.

Реализация модели:

![](code_xgb.jpg)

Все значения параметров были определены подбором.

Библиотека xgboost принесла наилучший результат - 1888 место с RMLSE = 0.13796.

![](xgb_position.jpg)

## Summary

На этот competition я потратил больше всего времени, потому это было мое первое соревнование на Kaggle и первые реализации моделей с ориентацией на лучший результат. Многому я учился на ходу, действовал методом проб и ошибок и в итоге получил следующие результаты:

* xgboost с экстраполяцией - лучший результат
* randomForest c интерполяцией на втором месте
* linear model с интерполяцией последний
